{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and global settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from spacy.lang.en import English\n",
    "#from spacytextblob.spacytextblob import SpacyTextBlob\n",
    "from collections import Counter\n",
    "import en_core_web_sm\n",
    "import de_core_news_sm\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk import word_tokenize\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "pd.set_option('display.max_rows', 100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape wikipedia website List of place names of German origin in the United States using BeautifulSoup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "url = 'https://en.wikipedia.org/wiki/List_of_place_names_of_German_origin_in_the_United_States'\n",
    "html = urlopen(url) \n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "tables = soup.find_all('table')\n",
    "\n",
    "place_names = []\n",
    "states = []\n",
    "origin_notes = []\n",
    "place = []\n",
    "cells_holder = []\n",
    "\n",
    "for table in tables:\n",
    "    rows = table.find_all('tr')\n",
    "    #print(rows)\n",
    "    \n",
    "    for row in rows:\n",
    "        cells = row.find_all('td')\n",
    "        #cells_holder.append(cells)\n",
    "       # print(cells)\n",
    "        \n",
    "        if len(cells) > 1:\n",
    "            try:\n",
    "                place = cells[0]\n",
    "                place_names.append(place.text.strip())\n",
    "            except:\n",
    "                print()\n",
    "        \n",
    "            try:\n",
    "                state = cells[1]\n",
    "                states.append(state.text.strip())\n",
    "  \n",
    "            except:\n",
    "                print()\n",
    "            try:\n",
    "                origin_note = cells[2]\n",
    "                origin_notes.append(origin_note.text.strip())        \n",
    "            except:\n",
    "                print()\n",
    "                \n",
    "place_names = place_names[1:]\n",
    "states = states[1:]\n",
    "#print(place_names)\n",
    "#print(len(place_names))\n",
    "#print(len(states))\n",
    "#print(states)\n",
    "#print(len(origin_notes))\n",
    "#print(origin_notes)\n",
    "\n",
    "\n",
    "## Re-create the list of German places as it is in Wikipedia\n",
    "df_german_places = pd.DataFrame()\n",
    "df_german_places['Place Name'] = place_names\n",
    "df_german_places['State'] = states\n",
    "df_german_places['Origin'] = origin_notes\n",
    "\n",
    "#df_german_places"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert extracted soup into something useful for the further analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load extracted data into pandas df \n",
    "## Extract person names and geographic place names from list of place names of German origin in the United States \n",
    "## Print persons of interest with count for further analysis\n",
    "\n",
    "## Initialize empty pd data frames\n",
    "df_extracted_original_places = pd.DataFrame()\n",
    "df_extracted_persons = pd.DataFrame()\n",
    "list_extracted_original_places=[]\n",
    "list_extracted_persons=[]\n",
    "\n",
    "## Extract natural persons and geographic entities\n",
    "# Convert origin notes from list to string  \n",
    "\n",
    "\n",
    "\n",
    "origin_notes_str = ' '.join([str(elem) for elem in origin_notes])\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc_origin_notes = nlp(origin_notes_str)\n",
    "\n",
    "\n",
    "for ent in doc_origin_notes.ents:\n",
    "    if ent.label_ == \"PERSON\":        \n",
    "        list_extracted_persons.append(ent.text)\n",
    "    elif ent.label_ == \"GPE\":\n",
    "        list_extracted_original_places.append(ent.text)\n",
    "\n",
    "## Make places unique\n",
    "list_extracted_original_places=list(set(list_extracted_original_places))        \n",
    "        \n",
    "\n",
    "df_extracted_original_places = pd.DataFrame({'Place Names': list_extracted_original_places})\n",
    "df_extracted_persons = pd.DataFrame({'Persons of Interest': list_extracted_persons})\n",
    "#df_extracted_original_places\n",
    "#df_extracted_persons\n",
    "#df_extracted_persons['Persons of Interest'].value_counts()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Print results of initial analysis for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Johann de Kalb                 6\n",
       "Alexander von Humboldt         4\n",
       "George III                     2\n",
       "Frederick Muhlenberg           2\n",
       "Fulda                          2\n",
       "Martin Luther Named            2\n",
       "William Hoehne                 1\n",
       "Brna                           1\n",
       "Kiel                           1\n",
       "Gustav Schleicher              1\n",
       "Francis Xavier Pierz           1\n",
       "Lenzburg                       1\n",
       "William Waldorf Astor          1\n",
       "Albert Etter                   1\n",
       "Otto Fischer                   1\n",
       "Dissen                         1\n",
       "John Kieler                    1\n",
       "Henry Wickenburg               1\n",
       "Adolph Hegewisch               1\n",
       "Prince Carl                    1\n",
       "Ludwig Börne                   1\n",
       "John Meiners                   1\n",
       "Compound Wald                  1\n",
       "Adelsverein                    1\n",
       "Otto von Bismarck              1\n",
       "Jonathan Hager                 1\n",
       "John A. Roebling               1\n",
       "P. J.                          1\n",
       "Kassel                         1\n",
       "Henry C. Lutkens[11]:77        1\n",
       "Saxe-Altenburg                 1\n",
       "Frederick \"The Great\" of       1\n",
       "Bingen                         1\n",
       "Carl Schurz                    1\n",
       "Rhein                          1\n",
       "Otto von Kotzebue              1\n",
       "Philip Deidesheimer            1\n",
       "Slavkov                        1\n",
       "Ferdinand von Wrangel          1\n",
       "Frederick                      1\n",
       "Charles William Ferdinand      1\n",
       "John Valentine Steger          1\n",
       "Baltimore                      1\n",
       "Kranz                          1\n",
       "Sebastian Kronenwetter         1\n",
       "Ontario                        1\n",
       "Joseph von Boos-Waldeck.[25    1\n",
       "Karlovy Vary                   1\n",
       "Frederick Calvert              1\n",
       "Rudolph Kremmling              1\n",
       "William Schley                 1\n",
       "von Steuben                    1\n",
       "Daniel Saeger                  1\n",
       "Silesia Neighborhood           1\n",
       "William Bremer                 1\n",
       "Hermann                        1\n",
       "Henry Rosenberg                1\n",
       "Brunswick                      1\n",
       "William Seaborn Bamberg        1\n",
       "Franz Sigel                    1\n",
       "Solms-Braunfels                1\n",
       "Frederick Louis                1\n",
       "Michael Dieterich              1\n",
       "Michael Ohlman                 1\n",
       "German Word                    1\n",
       "Name: Persons of Interest, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Print extracted persons of interest nad count their times of appearance in the origin description\n",
    "df_extracted_persons['Persons of Interest'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Original Places</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>California</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Colorado</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Karlsbad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ault</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Baden</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Otoe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Germany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Lützen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Nebraska</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Minden</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Hamburg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Great Britain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Hohenlinden</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>United States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Ulm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Jena</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Rensselaer County Named</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Bavaria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Fischer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Augsburg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Schaumburg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Switzerland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Czech Republic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Seattle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Flensburg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Washington Named</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Hector</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>South Stormont</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Osnabrück</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Altdorf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Denver</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Chicago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Westphalia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Karlsruhe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Santa Ana River</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Prussia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Genevra</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Friedheim</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>the Kingdom of Bavaria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>Vroman</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>Poland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>New York Named</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Leipzig</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>Melle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>Berlin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>Pirc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>Potsdam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>Georgia</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Original Places\n",
       "0                California\n",
       "1                  Colorado\n",
       "2                  Karlsbad\n",
       "3                      Ault\n",
       "4                     Baden\n",
       "5                      Otoe\n",
       "6                   Germany\n",
       "7                    Lützen\n",
       "8                  Nebraska\n",
       "9                    Minden\n",
       "10                  Hamburg\n",
       "11            Great Britain\n",
       "12              Hohenlinden\n",
       "13            United States\n",
       "14                      Ulm\n",
       "15                     Jena\n",
       "16  Rensselaer County Named\n",
       "17                  Bavaria\n",
       "18                  Fischer\n",
       "19                 Augsburg\n",
       "20               Schaumburg\n",
       "21              Switzerland\n",
       "22           Czech Republic\n",
       "23                  Seattle\n",
       "24                Flensburg\n",
       "25         Washington Named\n",
       "26                   Hector\n",
       "27           South Stormont\n",
       "28                Osnabrück\n",
       "29                  Altdorf\n",
       "30                   Denver\n",
       "31                  Chicago\n",
       "32               Westphalia\n",
       "33                Karlsruhe\n",
       "34          Santa Ana River\n",
       "35                  Prussia\n",
       "36                  Genevra\n",
       "37                Friedheim\n",
       "38   the Kingdom of Bavaria\n",
       "39                   Vroman\n",
       "40                   Poland\n",
       "41           New York Named\n",
       "42                  Leipzig\n",
       "43                    Melle\n",
       "44                   Berlin\n",
       "45                     Pirc\n",
       "46                  Potsdam\n",
       "47                  Georgia"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Print extracted original places\n",
    "df_extracted_original_places"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export place names to csv (uncomment only if new list needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_extracted_original_places.to_csv(r'c:\\temp\\extracted_original_places.csv', index = False, header = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the functions for further analysis of the websites that are scraped\n",
    "# based off the results of the initial scrape and investigation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Another spacy function for stop words removal third attempt\n",
    "def spacy_stop_word_remover(soup):\n",
    "    sp = spacy.load('en_core_web_sm')\n",
    "    all_stopwords = sp.Defaults.stop_words\n",
    "    text = soup\n",
    "    words = [word for word in text.split() if word.lower() not in all_stopwords]\n",
    "    new_text = \" \".join(words)\n",
    "    #print(new_text)\n",
    "    print(\"Old length: \", len(text))\n",
    "    print(\"New length: \", len(new_text))\n",
    "    return new_text\n",
    "\n",
    "\n",
    "## spacy lemmatizer function\n",
    "def spacy_lemmatize(list_to_lemmatize):\n",
    "    sp = spacy.load('en_core_web_sm')\n",
    "    text_to_lemma = sp(list_to_lemmatize)\n",
    "    text_to_return = \"\"\n",
    "    for word in text_to_lemma:\n",
    "        #print(word.text,  word.lemma_)\n",
    "        text_to_return=text_to_return + \" \" + word.lemma_\n",
    "    return text_to_return\n",
    "\n",
    "## Simple NLTK sentiment intensity analyser function\n",
    "def nltk_sentiment_analyser(text):\n",
    "    input_txt = text\n",
    "    \n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "    polscores=sia.polarity_scores(input_txt)\n",
    "    return polscores\n",
    "\n",
    "## Spacy sentiment analyser English\n",
    "\n",
    "def spacy_sentiment_analyser_en(text_to_analyse):\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    nlp.add_pipe('spacytextblob')\n",
    "    text=text_to_analyse\n",
    "    doc = nlp(text)\n",
    "    print(\"The polarity of the English text is: \" + str(doc._.polarity)) \n",
    "    print(\"The subjectivity of the English text is: \" + str(doc._.subjectivity))\n",
    "    #doc._.assessments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions German"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Stop words removal German\n",
    "def spacy_stop_word_remover_german(soup):\n",
    "    sp = spacy.load('de_core_news_sm')\n",
    "    all_stopwords = sp.Defaults.stop_words\n",
    "    text = soup\n",
    "    words = [word for word in text.split() if word.lower() not in all_stopwords]\n",
    "    new_text = \" \".join(words)\n",
    "    #print(new_text)\n",
    "    print(\"Old length: \", len(text))\n",
    "    print(\"New length: \", len(new_text))\n",
    "    return new_text\n",
    "\n",
    "\n",
    "## Spacy lemmatizer function German\n",
    "def spacy_lemmatize_german(list_to_lemmatize):\n",
    "    sp = spacy.load('de_core_news_sm')\n",
    "    text_to_lemma = sp(list_to_lemmatize)\n",
    "    text_to_return = \"\"\n",
    "    for word in text_to_lemma:\n",
    "        #print(word.text,  word.lemma_)\n",
    "        text_to_return=text_to_return + \" \" + word.lemma_\n",
    "    return text_to_return\n",
    "\n",
    "## Spacy sentiment analyser German\n",
    "def spacy_sentiment_analyser_de(text_to_analyse):\n",
    "    nlp = spacy.load('de_core_news_sm')\n",
    "    nlp.add_pipe('spacytextblob')\n",
    "    text=text_to_analyse\n",
    "    doc = nlp(text)\n",
    "    print(\"The polarity of the German text is: \" + str(doc._.polarity))\n",
    "    print(\"The subjectivity of the German text is: \" + str(doc._.subjectivity))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GIT Connection and upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reinitialized existing Git repository in C:/Users/hgc24/OneDrive/Master_Data_Science/Data Science Master Class 1/ass3/.git/\n"
     ]
    }
   ],
   "source": [
    "## GIT Connect\n",
    "# Set remote origin\n",
    "!git init  \n",
    "#!git remote add origin https://github.com/HGC243/Assignment_3.git\n",
    "!git remote set-url origin https://github.com/HGC243/Assignment_3.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status:\n",
      "On branch master\n",
      "Changes not staged for commit:\n",
      "  (use \"git add <file>...\" to update what will be committed)\n",
      "  (use \"git restore <file>...\" to discard changes in working directory)\n",
      "\tmodified:   .ipynb_checkpoints/Assignment_3_Code_Holger_Colin-checkpoint.ipynb\n",
      "\tmodified:   Assignment_3_Code_Holger_Colin.ipynb\n",
      "\n",
      "no changes added to commit (use \"git add\" and/or \"git commit -a\")\n",
      "Files to add (This will be empty if there are no new files):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "warning: LF will be replaced by CRLF in .ipynb_checkpoints/Assignment_3_Code_Holger_Colin-checkpoint.ipynb.\n",
      "The file will have its original line endings in your working directory\n",
      "warning: LF will be replaced by CRLF in Assignment_3_Code_Holger_Colin.ipynb.\n",
      "The file will have its original line endings in your working directory\n"
     ]
    }
   ],
   "source": [
    "## Check GIT Status\n",
    "print('Status:')\n",
    "!git status\n",
    "\n",
    "# Add files to GIT\n",
    "print('Files to add (This will be empty if there are no new files):')\n",
    "!git add ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[master 7e8f66a] End of day 15/04/2021 commit\n",
      " 2 files changed, 2714 insertions(+), 1416 deletions(-)\n",
      " rewrite .ipynb_checkpoints/Assignment_3_Code_Holger_Colin-checkpoint.ipynb (72%)\n",
      " rewrite Assignment_3_Code_Holger_Colin.ipynb (72%)\n"
     ]
    }
   ],
   "source": [
    "## Git Commit\n",
    "#!git commit -m \"End of day dd/mm/2021 commit\"\n",
    "!git commit -m \"End of day 15/04/2021 commit\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To https://github.com/HGC243/Assignment_3.git\n",
      "   5abf346..7e8f66a  master -> master\n"
     ]
    }
   ],
   "source": [
    "## Git push all\n",
    "!git push --all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape further websites based off the results of the initial investigations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape, cleanse and analyse Martin Luther Wikipedia site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Scrape website Martin Luther (English)\n",
    "url_ml = 'https://en.wikipedia.org/wiki/Martin_Luther'\n",
    "html_ml = urlopen(url_ml) \n",
    "soup_ml = BeautifulSoup(html_ml, 'html.parser').get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Scrape website Martin Luther (German)\n",
    "url_ml_de = 'https://de.wikipedia.org/wiki/Martin_Luther'\n",
    "html_ml_de = urlopen(url_ml_de) \n",
    "soup_ml_de = BeautifulSoup(html_ml_de, 'html.parser').get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Old length:  144714\n",
      "New length:  115136\n",
      "The polarity of the English text is: 0.05408380164687502\n",
      "The subjectivity of the English text is: 0.35739486049119074\n",
      "The polarity scores are:\n",
      "\n",
      "{'neg': 0.085, 'neu': 0.825, 'pos': 0.091, 'compound': 0.9753}\n"
     ]
    }
   ],
   "source": [
    "## Steps to cleanse scraped Wikipedia site Martin Luther (English)\n",
    "## 1. Call stop word remover\n",
    "no_stop_words_ml=spacy_stop_word_remover(soup_ml)\n",
    "\n",
    "## 2. Call lemmatizer\n",
    "ml_lemmatized = spacy_lemmatize(no_stop_words_ml)\n",
    "\n",
    "## 4. Run sentiment analysis Spacy\n",
    "spacy_sentiment_analyser_en(ml_lemmatized)  \n",
    "\n",
    "## 4. Run sentiment analysis NLTK (English only)\n",
    "ml_sentiment=nltk_sentiment_analyser(ml_lemmatized)\n",
    "print(\"The polarity scores are:\\n\")\n",
    "print(ml_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Old length:  174091\n",
      "New length:  133974\n",
      "The polarity of the German text is: 0.02128378378378378\n",
      "The subjectivity of the German text is: 0.3673423423423423\n"
     ]
    }
   ],
   "source": [
    "## Steps to cleanse scraped Wikipedia site Martin Luther (German)\n",
    "no_stop_words_ml_de =spacy_stop_word_remover_german(soup_ml_de)\n",
    "ml_lemmatized_de = spacy_lemmatize_german(no_stop_words_ml_de)\n",
    "spacy_sentiment_analyser_de(ml_lemmatized_de)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape, cleanse and analyse Otto von Bismarck Wikipedia site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Scrape website Otto von Bismarck (English)\n",
    "url_ovb = 'https://en.wikipedia.org/wiki/Otto_von_Bismarck'\n",
    "html_ovb = urlopen(url_ovb) \n",
    "soup_ovb = BeautifulSoup(html_ovb, 'html.parser').get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Scrape website Otto von Bismarck (German)\n",
    "url_ovb_de = 'https://de.wikipedia.org/wiki/Otto_von_Bismarck'\n",
    "html_ovb_de = urlopen(url_ovb_de) \n",
    "soup_ovb_de = BeautifulSoup(html_ovb_de, 'html.parser').get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Old length:  142473\n",
      "New length:  111623\n",
      "The polarity of the English text is: 0.07353583132348823\n",
      "The subjectivity of the English text is: 0.3385020426652225\n",
      "The polarity scores are:\n",
      "\n",
      "{'neg': 0.108, 'neu': 0.766, 'pos': 0.126, 'compound': 0.9997}\n"
     ]
    }
   ],
   "source": [
    "## Steps to cleanse scraped Wikipedia site Otto von Bismarck (English)\n",
    "no_stop_words_ovb=spacy_stop_word_remover(soup_ovb)\n",
    "ovb_lemmatized = spacy_lemmatize(no_stop_words_ovb)\n",
    "spacy_sentiment_analyser_en(ovb_lemmatized)  \n",
    "ovb_sentiment=nltk_sentiment_analyser(ovb_lemmatized)\n",
    "print(\"The polarity scores are:\\n\")\n",
    "print(ovb_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Old length:  161404\n",
      "New length:  158798\n",
      "The polarity of the German text is: 0.06275252525252527\n",
      "The subjectivity of the German text is: 0.40881895881895874\n",
      "The polarity scores are:\n",
      "\n",
      "{'neg': 0.009, 'neu': 0.986, 'pos': 0.005, 'compound': -0.9974}\n"
     ]
    }
   ],
   "source": [
    "## Steps to cleanse scraped Wikipedia site Otto von Bismarck (German)\n",
    "no_stop_words_ovb_de=spacy_stop_word_remover(soup_ovb_de)\n",
    "ovb_de_lemmatized = spacy_lemmatize_german(no_stop_words_ovb_de)\n",
    "spacy_sentiment_analyser_de(ovb_de_lemmatized)  \n",
    "ovb_de_sentiment=nltk_sentiment_analyser(ovb_de_lemmatized)\n",
    "print(\"The polarity scores are:\\n\")\n",
    "print(ovb_de_sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape, cleanse and analyse Alexander von Humboldt Wikipedia site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Scrape website Alexander von Humboldt (English)\n",
    "url_avh = 'https://en.wikipedia.org/wiki/Alexander_von_Humboldt'\n",
    "html_avh = urlopen(url_avh) \n",
    "soup_avh = BeautifulSoup(html_avh, 'html.parser').get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Scrape website Alexander von Humboldt (German)\n",
    "url_avh_de = 'https://de.wikipedia.org/wiki/Alexander_von_Humboldt'\n",
    "html_avh_de = urlopen(url_avh_de) \n",
    "soup_avh_de = BeautifulSoup(html_avh_de, 'html.parser').get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Old length:  150924\n",
      "New length:  121614\n",
      "The polarity of the English text is: 0.08755867451903791\n",
      "The subjectivity of the English text is: 0.3547645066594741\n",
      "The polarity scores are:\n",
      "\n",
      "{'neg': 0.033, 'neu': 0.875, 'pos': 0.092, 'compound': 1.0}\n"
     ]
    }
   ],
   "source": [
    "## Steps to cleanse scraped Wikipedia site Otto von Bismarck (English)\n",
    "no_stop_words_avh=spacy_stop_word_remover(soup_avh)\n",
    "avh_lemmatized = spacy_lemmatize(no_stop_words_avh)\n",
    "spacy_sentiment_analyser_en(avh_lemmatized)  \n",
    "avh_sentiment=nltk_sentiment_analyser(avh_lemmatized)\n",
    "print(\"The polarity scores are:\\n\")\n",
    "print(avh_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Old length:  193665\n",
      "New length:  190585\n",
      "The polarity of the German text is: 0.04963556014692378\n",
      "The subjectivity of the German text is: 0.25022956841138644\n",
      "The polarity scores are:\n",
      "\n",
      "{'neg': 0.012, 'neu': 0.98, 'pos': 0.008, 'compound': -0.9978}\n"
     ]
    }
   ],
   "source": [
    "## Steps to cleanse scraped Wikipedia site Otto von Bismarck (German)\n",
    "no_stop_words_avh_de=spacy_stop_word_remover(soup_avh_de)\n",
    "avh_de_lemmatized = spacy_lemmatize_german(no_stop_words_avh_de)\n",
    "spacy_sentiment_analyser_de(avh_de_lemmatized)  \n",
    "avh_de_sentiment=nltk_sentiment_analyser(avh_de_lemmatized)\n",
    "print(\"The polarity scores are:\\n\")\n",
    "print(avh_de_sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape, cleanse List of German place names Wikipedia site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Scrape website list of places in Germany (German)\n",
    "url_lpg_de = 'https://de.wikipedia.org/wiki/Liste_der_St%C3%A4dte_in_Deutschland'\n",
    "html_lpg_de = urlopen(url_lpg_de) \n",
    "soup_lpg_de = BeautifulSoup(html_lpg_de, 'html.parser')\n",
    "tables = soup_lpg_de.find_all('table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Convert content of website into cleansed list of place names\n",
    "place_name_holder=[]\n",
    "for table in tables:\n",
    "    rows = table.find_all('tr')\n",
    "    #print(rows)\n",
    "        \n",
    "\n",
    "    for row in rows:\n",
    "        place_lines = row.find_all('dd')\n",
    "        #place_name_holder.append(place_lines)\n",
    "        #place_name_holder = place_name_holder + \" \" + place_lines\n",
    "        #print(place_lines)\n",
    "        \n",
    "        for names in place_lines:\n",
    "            place_names = names.find_all('a')\n",
    "            \n",
    "            \n",
    "            to_string=str(place_names)\n",
    "            extracted_place=re.findall(r'>([^\"]*)<', to_string)\n",
    "            #print(place_names)\n",
    "            #print(extracted_place)\n",
    "            place_name_holder.append(extracted_place)\n",
    "            \n",
    "df_allplaces_Germany = pd.DataFrame(place_name_holder)\n",
    "df_allplaces_Germany=df_allplaces_Germany.rename(columns={0: 'Place Names'})\n",
    "#df_allplaces_Germany"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Place Names</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Augsburg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Berlin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Flensburg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hamburg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Jena</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Karlsruhe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Leipzig</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Lützen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Melle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Minden</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Osnabrück</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Potsdam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Ulm</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Place Names\n",
       "0     Augsburg\n",
       "1       Berlin\n",
       "2    Flensburg\n",
       "3      Hamburg\n",
       "4         Jena\n",
       "5    Karlsruhe\n",
       "6      Leipzig\n",
       "7       Lützen\n",
       "8        Melle\n",
       "9       Minden\n",
       "10   Osnabrück\n",
       "11     Potsdam\n",
       "12         Ulm"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Compare both German place name lists and output matching records only \n",
    "verfied_place_names = pd.merge(df_allplaces_Germany, df_extracted_original_places, how='inner', on=['Place Names'])\n",
    "verfied_place_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unused draft functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## I have left these here just to show how much time I have spent working through this whilst trying things out\n",
    "## This doesn't look like much but all of these failed attempts to get functions to work cost lots of time\n",
    "## This is only the second subject in Python and the first one is a while back so the learning curve is steep\n",
    "\n",
    "#def spacy_stop_word_remover(soup):\n",
    "#    sp = spacy.load('en_core_web_sm')\n",
    "#    soup_for_processing = soup\n",
    "#    all_stopwords = sp.Defaults.stop_words\n",
    "#    text_tokens = word_tokenize(soup_for_processing)\n",
    "#    tokens_without_sw= [word for word in text_tokens if not word in all_stopwords]\n",
    "#    return tokens_without_sw\n",
    "\n",
    "\n",
    "### NOT WORKING WELL\n",
    "\n",
    "#\n",
    "#   # Create list of word tokens\n",
    "#    token_list = []\n",
    "#    for token in doc:        \n",
    "#        token_list.append(token.text)\n",
    "#\n",
    "#    filtered_text =[] \n",
    "#\n",
    "#    for word in token_list:\n",
    "#        lexeme = nlp.vocab[word]\n",
    "#        if lexeme.is_stop == False:\n",
    "#            #print(word)\n",
    "#            filtered_text.append(word) \n",
    "#\n",
    "#    return filtered_text\n",
    "\n",
    "\n",
    "\n",
    "## Simple lemmatizer Function \n",
    "def simple_lemmatizer(list_of_words):\n",
    "    to_lemmatize=list_of_words\n",
    "    lmtzr = WordNetLemmatizer()\n",
    "    lemmatized = [[lmtzr.lemmatize(word) for word in word_tokenize(s)]for s in to_lemmatize]\n",
    "    return lemmatized\n",
    "#print(lemmatized)\n",
    "\n",
    "# getting length of list\n",
    "#length = len(lemmatized)\n",
    "#data = []  \n",
    "\n",
    "#for i in range(length):\n",
    "#    string = ' '.join(lemmatized[i])\n",
    "#    #print(string)\n",
    "#    data.append(string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
